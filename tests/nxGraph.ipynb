{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas\n",
    "from hatchet import *\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import platform\n",
    "import json\n",
    "from ast import literal_eval as make_tuple\n",
    "from state import State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "plt.rcParams['figure.figsize'] = (16, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the datasets and create Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'kripke-mvapich2'\n",
    "dir_name = \"/Users/jarus/ucd/Research/Visualisation/projects/CallFlow/.callflow/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_str_with_Node(df, graph):\n",
    "    mapper = {}\n",
    "    def dfs_recurse(root):\n",
    "        for node in root.children: \n",
    "            mapper[node.callpath[-1]] = Node(node.nid, node.callpath, None)\n",
    "            dfs_recurse(node)\n",
    "    for root in graph.roots:\n",
    "        mapper[root.callpath[-1]] = Node(root.nid, root.callpath, None)\n",
    "        dfs_recurse(root)\n",
    "    df['node'] = df['node'].apply(lambda node: mapper[node] if node in mapper else '')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gf(name):\n",
    "    state = State()\n",
    "    path = dir_name + dataset\n",
    "    df_filepath = path + '/group_df.csv'\n",
    "    entire_df_filepath = path + '/entire_df.csv'\n",
    "    graph_filepath = path + '/filter_graph.json'\n",
    "    entire_graph_filepath = path + '/entire_graph.json'   \n",
    "\n",
    "    with open(graph_filepath, 'r') as graphFile:\n",
    "        data = json.load(graphFile)\n",
    "\n",
    "    state.gf = GraphFrame()\n",
    "    state.gf.from_literal(data)\n",
    "\n",
    "    with open(entire_graph_filepath, 'r') as entire_graphFile:\n",
    "        entire_data = json.load(entire_graphFile)\n",
    "            \n",
    "    state.entire_gf = GraphFrame()\n",
    "    state.entire_gf.from_literal(entire_data)\n",
    "\n",
    "    state.df = pd.read_csv(df_filepath)\n",
    "    state.entire_df = pd.read_csv(entire_df_filepath)\n",
    "\n",
    "    state.graph = state.gf.graph\n",
    "    state.entire_graph = state.entire_gf.graph\n",
    "\n",
    "\n",
    "    # replace df['node'] from str to the Node object.\n",
    "    state.df = replace_str_with_Node(state.df, state.graph)\n",
    "    state.entire_df = replace_str_with_Node(state.entire_df, state.entire_graph)\n",
    "\n",
    "    # add path to the dataframes. \n",
    "    # state.df['path'] = state.df['node'].apply(lambda node: node.callpath)\n",
    "    # state.entire_df['path'] = state.entire_df['node'].apply(lambda node: node.callpath if node else [])\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = {}\n",
    "states[name] = read_gf(name)\n",
    "df = states[dataset].df\n",
    "graph = states[dataset].graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gets the paths of functions inside the module and converts to a dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHierarchyfromdf(state, module):\n",
    "    df = state.df\n",
    "    paths = []\n",
    "    func_in_module = df.loc[df['module'] == module]['name'].unique().tolist()\n",
    "    print(\"Number of functions inside the {0} module: {1}\".format(module, len(func_in_module)))\n",
    "    for idx, func in enumerate(func_in_module):\n",
    "        mean_inc_time = df.loc[df['name'] == func]['time (inc)'].mean()\n",
    "        mean_exc_time = df.loc[df['name'] == func]['time'].mean()\n",
    "        paths.append({\n",
    "            \"module\": module,\n",
    "            \"opath\": df.loc[df['name'] == func]['path'].unique().tolist()[0],\n",
    "            \"path\": df.loc[df['name'] == func]['component_path'].unique().tolist()[0],\n",
    "            \"inc_time\" : df.loc[df['name'] == func]['time (inc)'].mean(),\n",
    "            \"exclusive\" : df.loc[df['name'] == func]['time'].mean(),\n",
    "#             \"imbalance_perc\" : df.loc[df['name'] == func]['imbalance_perc'].mean(),\n",
    "            \"component_level\": df.loc[df['name'] == func]['component_level'].unique().tolist()[0],\n",
    "        })\n",
    "    return pd.DataFrame(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of functions inside the libmonitor.so.0.0.0 module: 1\n",
      "Number of functions inside the kripke module: 10\n",
      "Number of functions inside the Unknown(NA) module: 2\n",
      "Number of functions inside the libc-2.17.so module: 1\n",
      "Number of functions inside the libmpi.so.12.0.5 module: 9\n"
     ]
    }
   ],
   "source": [
    "for idx, state_name in enumerate(states):\n",
    "    state = states[state_name]\n",
    "    modules = state.df['module'].unique().tolist()\n",
    "    for idx, module in enumerate(modules):\n",
    "        paths = getHierarchyfromdf(state, module)\n",
    "        state.paths_df = paths \n",
    "        #paths.to_csv(str(module + \".csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods to add data into NxGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_levels(state):\n",
    "    levelMap = {}\n",
    "    track_level = 0\n",
    "    nodes = state.g.nbunch_iter(state.root)\n",
    "    \n",
    "    for start_node in nodes:\n",
    "        print(\"Start node\", start_node)\n",
    "        active_nodes = [start_node]\n",
    "        levelMap[state.root] = 0\n",
    "        \n",
    "        for edge in nx.edge_dfs(state.g, start_node, 'original'):\n",
    "            #rint(\"Edge {0}\".format(edge))\n",
    "            head_level = None\n",
    "            tail_level = None\n",
    "            head, tail = edge[0], edge[1]\n",
    "            \n",
    "            if head != start_node:\n",
    "                active_nodes.append(head)\n",
    "                \n",
    "            if head in active_nodes and head != start_node and tail in active_nodes:\n",
    "                #rint(\"Cycle\", edge)\n",
    "                edge_data = state.g.get_edge_data(*edge)                                                                             \n",
    "                state.g.add_node(tail+'_')                                                                                           \n",
    "                state.g.add_edge(head, tail+'_', data=edge_data)                                                                     \n",
    "                state.g.node[tail+'_']['name'] = [tail + '_']                                                                        \n",
    "                #state.g.node[tail+'_']['weight'] = state.g.node[tail]['weight']   \n",
    "                state.g.remove_edge(edge[0], edge[1])\n",
    "    return levelMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    " def flow_map(state):                                                                                                                 \n",
    "        flowMap = {}                                                                                                                    \n",
    "        nodes = state.g.nbunch_iter(state.root)                                                                                           \n",
    "        for start_node in nodes:                                                                                                        \n",
    "            for edge in nx.edge_dfs(state.g, start_node, 'original'):                                                                    \n",
    "                head_level = None                                                                                                       \n",
    "                tail_level = None                                                                                                       \n",
    "                head, tail = self.tailhead(edge)                                                                                        \n",
    "                                                                                                                                        \n",
    "                # Check if there is an existing level mapping for the head node and assign.                                             \n",
    "                if head in self.level_mapping.keys():                                                                                   \n",
    "                    head_level =  self.level_mapping[head]                                                                              \n",
    "                                                                                                                                        \n",
    "                # Check if there is an existing level mapping for the tail node and assign.                                             \n",
    "                if tail in self.level_mapping.keys():                                                                                   \n",
    "                    tail_level = self.level_mapping[tail]                                                                               \n",
    "                                                                                                                                        \n",
    "                flowMap[(edge[0], edge[1])] = (int(head_level), int(tail_level))                                                        \n",
    "        return flowMap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_flows(state):\n",
    "    graph = state.g\n",
    "    ret = {}                                                                                                                                                                                                                                                                          \n",
    "    edges = graph.edges()                                                                                                                                                                                                                                                             \n",
    "    additional_flow = {}                                                                                                                                                                                                                                                              \n",
    "                                                                                                                                                                                                                                                                                          \n",
    "    # Calculates the costs in cycles and aggregates to one node.                                                                                                                                                                                                                      \n",
    "    for edge in edges:                                                                                                                                                                                                                                                                \n",
    "        source = edge[0]                                                                                                                                                                                                                                                              \n",
    "        target = edge[1]                                                                                                                                                                                                                                                              \n",
    "                                                                                                                                                                                                                                                                                          \n",
    "        if source.endswith('_'):                                                                                                                                                                                                                                                      \n",
    "            cycle_node = source                                                                                                                                                                                                                                                       \n",
    "            cycle_node_df = self.state.lookup_with_nodeName(cycle_node[:-1])                                                                                                                                                                                                      \n",
    "            additional_flow[cycle_node] = cycle_node_df['CPUTIME (usec) (I)'].max()                                                                                                                                                                                                   \n",
    "        elif target.endswith('_'):                                                                                                                                                                                                                                                    \n",
    "            cycle_node = target                                                                                                                                                                                                                                                       \n",
    "            cycle_node_df = state.lookup_with_nodeName(cycle_node[:-1])                                                                                                                                                                                                      \n",
    "            additional_flow[cycle_node] = cycle_node_df['CPUTIME (usec) (I)'].max()                                                                                                                                                                                                   \n",
    "                                                                                                                                                                                                                                                                                          \n",
    "    for edge in edges:                                                                                                                                                                                                                                                                \n",
    "        added_flow = 0                                                                                                                                                                                                                                                                \n",
    "        if edge[0].endswith('_'):                                                                                                                                                                                                                                                     \n",
    "            ret[edge] = additional_flow[edge[0]]                                                                                                                                                                                                                                      \n",
    "            continue                                                                                                                                                                                                                                                                  \n",
    "        elif edge[1].endswith('_'):                                                                                                                                                                                                                                                   \n",
    "            ret[edge] = additional_flow[edge[1]]                                                                                                                                                                                                                                      \n",
    "            continue                                                                                                                                                                                                                                                                  \n",
    "        source = state.lookup_with_nodeName(edge[0])                                                                                                                                                                                                                         \n",
    "        target = state.lookup_with_nodeName(edge[1])                                                                                                                                                                                                                         \n",
    "                                                                                                                                                                                                                                                                                          \n",
    "        source_inc = source['time (inc)'].max()                                                                                                                                                                                                                               \n",
    "        target_inc = target['time (inc)'].max()                                                                         \n",
    "                                                                                                                                                                                                                                                                                          \n",
    "        if source_inc == target_inc:                                                                                                                                                                                                                                                  \n",
    "            ret[edge] = source_inc                                                                                                                                                                                                                                                    \n",
    "        else:                                                                                                                                                                                                                                                                         \n",
    "            ret[edge] = target_inc    \n",
    "    return ret   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_edge_attributes(state):\n",
    "    capacity_mapping = calculate_flows(state)    \n",
    "    nx.set_edge_attributes(state.g, name='weight', values=capacity_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_node_attributes(state):\n",
    "    module_mapping = generic_map(state.df, state.g.nodes(), 'module')\n",
    "    nx.set_node_attributes(state.g, name='module', values=module_mapping)\n",
    "    \n",
    "    time_inc_mapping = generic_map(state.df, state.g.nodes(), 'time (inc)')\n",
    "    nx.set_node_attributes(state.g, name='time (inc)', values=time_inc_mapping)\n",
    "\n",
    "    time_mapping = generic_map(state.df, state.g.nodes(), 'time')\n",
    "    nx.set_node_attributes(state.g, name=\"time\", values=time_mapping)\n",
    "\n",
    "#     name_mapping = generic_map(state.df, state.g.nodes(), 'vis_node_name')\n",
    "#     nx.set_node_attributes(state.g, name='name', values=name_mapping)\n",
    "\n",
    "#     type_mapping = generic_map(state.df, state.g.nodes(), 'type')\n",
    "#     nx.set_node_attributes(state.g, name='type', values=type_mapping)\n",
    "\n",
    "#     n_index_mapping = generic_map(state.df, state.g.nodes(), 'n_index')\n",
    "#     nx.set_node_attributes(state.g, name='n_index', values=n_index_mapping)\n",
    "\n",
    "#     module_mapping = generic_map(state.df, state.g.nodes(), 'module')\n",
    "#     nx.set_node_attributes(state.g, name='module', values=module_mapping)\n",
    "\n",
    "#     mod_index_mapping = generic_map(state.df, state.g.nodes(), 'mod_index')\n",
    "#     nx.set_node_attributes(state.g, name='mod_index', values=mod_index_mapping)\n",
    "\n",
    "#     children_mapping = immediate_children()\n",
    "#     nx.set_node_attributes(self.g, name='children', values=children_mapping)\n",
    "        \n",
    "#     entry_function_mapping = self.generic_map(self.g.nodes(), 'entry_functions')\n",
    "#     nx.set_node_attributes(self.g, name='entry_functions', values=entry_function_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_cycle_path(path):\n",
    "    ret = []\n",
    "    mapper = {}\n",
    "    path = make_tuple(path)\n",
    "    for idx, elem in enumerate(path):\n",
    "        if elem not in mapper:\n",
    "            mapper[elem] = 1\n",
    "            ret.append(elem)\n",
    "        else:\n",
    "            ret.append(elem + \"_\" + str(mapper[elem]))\n",
    "            mapper[elem] += 1\n",
    "    return tuple(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_paths(state, path_name):\n",
    "    for idx, row in state.df.iterrows():\n",
    "        path = row[path_name]\n",
    "        corrected_path = no_cycle_path(path)\n",
    "        state.g.add_path(corrected_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create NxGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libmonitor.so.0.0.0\n",
      "kripke\n",
      "Unknown(NA)\n",
      "kripke:Kernel_3d_DGZ::LTimes\n",
      "libc-2.17.so\n",
      "kripke:Kernel_3d_DGZ::scattering\n",
      "kripke:Kernel_3d_DGZ::LPlusTimes\n",
      "kripke:SweepSubdomains\n",
      "Unknown(NA):Loop@<unknown file> [kripke]:0\n",
      "kripke:SweepComm::readySubdomains\n",
      "Unknown(NA):Loop@<unknown file> [kripke]:0_1\n",
      "libmpi.so.12.0.5\n",
      "kripke:Kernel_3d_DGZ::sweep\n",
      "kripke:Grid_Data::particleEdit\n"
     ]
    }
   ],
   "source": [
    "def callgraph_init():\n",
    "    state = states[name]\n",
    "    state.g = nx.DiGraph()\n",
    "    state.root = state.lookup_with_node(state.graph.roots[0])['name'][0]\n",
    "    state.rootInc = state.lookup_with_node(state.graph.roots[0])['time (inc)'].max()\n",
    "        \n",
    "    add_paths(state, 'group_path')\n",
    "    add_node_attributes(state)\n",
    "    \n",
    "callgraph_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generic_map(df, nodes, attr):\n",
    "        ret = {}\n",
    "        group_by = 'module'\n",
    "        for node in nodes:\n",
    "            if attr == 'time (inc)':\n",
    "                if group_by == 'module':\n",
    "                    group_df = df.groupby([group_by]).mean()\n",
    "                elif group_by == 'name':\n",
    "                    group_df = df.groupby([group_by]).mean()\n",
    "                    \n",
    "                if ':' in node:\n",
    "                    corrected_node = node.split(':')[0]\n",
    "                else:\n",
    "                    corrected_node = node\n",
    "                print(node)\n",
    "                \n",
    "                ret[node] = group_df.loc[corrected_node, 'time (inc)']\n",
    "            \n",
    "            elif attr == 'entry_functions':\n",
    "                module_df = df.loc[df['module'] == node]\n",
    "                entry_func_df = module_df.loc[(module_df['component_level'] == 2)]\n",
    "                if(entry_func_df.empty):\n",
    "                    ret[node] = json.dumps({\n",
    "                        'name': '',\n",
    "                        'time': [],\n",
    "                        'time (inc)': []\n",
    "                    })\n",
    "                else:\n",
    "                    name = entry_func_df['name'].unique().tolist()\n",
    "                    time = entry_func_df['time'].mean().tolist()\n",
    "                    time_inc = entry_func_df['time (inc)'].mean().tolist()\n",
    "                \n",
    "                    ret[node] = json.dumps({\n",
    "                        'name': entry_func_df['name'].unique().tolist(),\n",
    "                        'time': entry_func_df['time'].mean().tolist(),\n",
    "                        'time (inc)': entry_func_df['time (inc)'].mean().tolist()\n",
    "                    })\n",
    "\n",
    "            elif attr == 'imbalance_perc':\n",
    "                module_df = df.loc[df['module'] == node]\n",
    "                max_incTime = module_df['time (inc)'].max()\n",
    "                min_incTime = module_df['time (inc)'].min()\n",
    "                avg_incTime = module_df['time (inc)'].mean()\n",
    "\n",
    "                ret[node] = (max_incTime - avg_incTime)/max_incTime\n",
    "\n",
    "            elif attr == 'time':\n",
    "                module_df = df.loc[df['module'] == node]\n",
    "                if group_by == 'module':\n",
    "                    group_df = df.groupby([group_by]).max()\n",
    "                elif self.group_by == 'name':\n",
    "                    group_df = df.groupby([group_by]).mean()\n",
    "                    \n",
    "                if ':' in node:\n",
    "                    corrected_node = node.split(':')[0]\n",
    "                else:\n",
    "                    corrected_node = node\n",
    "                ret[node] = group_df.loc[corrected_node, 'time']\n",
    "                \n",
    "            elif attr == 'module':\n",
    "                module_df = df.loc[df[group_by] == node][attr]\n",
    "                ret[node] = list(set(module_df.tolist()))\n",
    "                \n",
    "            else:\n",
    "                df = df.loc[df['vis_node_name'] == node][attr]\n",
    "                if df.empty:\n",
    "                    ret[node] = df[df[groupby] == node][attr]\n",
    "                else:\n",
    "                    ret[node] = list(set(df[df['vis_node_name'] == node][attr].tolist()))            \n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('libmonitor.so.0.0.0', {'module': ['libmonitor.so.0.0.0'], 'time (inc)': 194516926.140625, 'time': 0.0})\n",
      "('kripke', {'module': ['kripke'], 'time (inc)': 65521685.884183235, 'time': 78005891.0})\n",
      "('Unknown(NA)', {'module': ['Unknown(NA)'], 'time (inc)': 43454729.45870536, 'time': 61528596.0})\n",
      "('kripke:Kernel_3d_DGZ::LTimes', {'module': [], 'time (inc)': 65521685.884183235, 'time': 78005891.0})\n",
      "('libc-2.17.so', {'module': ['libc-2.17.so'], 'time (inc)': 2276253.6171875, 'time': 0.0})\n",
      "('kripke:Kernel_3d_DGZ::scattering', {'module': [], 'time (inc)': 65521685.884183235, 'time': 78005891.0})\n",
      "('kripke:Kernel_3d_DGZ::LPlusTimes', {'module': [], 'time (inc)': 65521685.884183235, 'time': 78005891.0})\n",
      "('kripke:SweepSubdomains', {'module': [], 'time (inc)': 65521685.884183235, 'time': 78005891.0})\n",
      "('Unknown(NA):Loop@<unknown file> [kripke]:0', {'module': [], 'time (inc)': 43454729.45870536, 'time': 61528596.0})\n",
      "('kripke:SweepComm::readySubdomains', {'module': [], 'time (inc)': 65521685.884183235, 'time': 78005891.0})\n",
      "('Unknown(NA):Loop@<unknown file> [kripke]:0_1', {'module': [], 'time (inc)': 43454729.45870536, 'time': 61528596.0})\n",
      "('libmpi.so.12.0.5', {'module': ['libmpi.so.12.0.5'], 'time (inc)': 2578231.9444444445, 'time': 0.0})\n",
      "('kripke:Kernel_3d_DGZ::sweep', {'module': [], 'time (inc)': 65521685.884183235, 'time': 78005891.0})\n",
      "('kripke:Grid_Data::particleEdit', {'module': [], 'time (inc)': 65521685.884183235, 'time': 78005891.0})\n"
     ]
    }
   ],
   "source": [
    "for node in states[dataset].g.nodes(data=True):\n",
    "    print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       libmonitor.so.0.0.0\n",
      "1       libmonitor.so.0.0.0\n",
      "2       libmonitor.so.0.0.0\n",
      "3       libmonitor.so.0.0.0\n",
      "4       libmonitor.so.0.0.0\n",
      "               ...         \n",
      "5246            Unknown(NA)\n",
      "5247            Unknown(NA)\n",
      "5248            Unknown(NA)\n",
      "5249            Unknown(NA)\n",
      "5250            Unknown(NA)\n",
      "Name: vis_node_name, Length: 5251, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(states[dataset].df['vis_node_name'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
